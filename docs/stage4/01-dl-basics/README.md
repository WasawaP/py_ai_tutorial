# æ¨¡å—M01: æ·±åº¦å­¦ä¹ åŸºç¡€ç†è®º

**é˜¶æ®µ**: Stage 4 - æ·±åº¦å­¦ä¹ 
**é¢„è®¡å­¦ä¹ æ—¶é—´**: 2-3å°æ—¶ï¼ˆç†è®ºï¼‰+ 2-3å°æ—¶ï¼ˆå®è·µï¼‰
**éš¾åº¦**: â­â­â­ ä¸­ç­‰

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- âœ… ç†è§£ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†ï¼ˆæ„ŸçŸ¥æœºã€å¤šå±‚æ„ŸçŸ¥æœºã€æ¿€æ´»å‡½æ•°ï¼‰
- âœ… æŒæ¡åå‘ä¼ æ’­ç®—æ³•çš„æ•°å­¦æ¨å¯¼ä¸è®¡ç®—è¿‡ç¨‹
- âœ… ç†Ÿæ‚‰å¸¸ç”¨ä¼˜åŒ–å™¨ï¼ˆSGDã€Adamã€RMSpropï¼‰åŠå…¶é€‚ç”¨åœºæ™¯
- âœ… ç†è§£æŸå¤±å‡½æ•°çš„é€‰æ‹©ä¸æ­£åˆ™åŒ–æŠ€æœ¯
- âœ… èƒ½å¤Ÿä½¿ç”¨PyTorchä»é›¶å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ
- âœ… æŒæ¡å¼ é‡æ“ä½œã€è‡ªåŠ¨å¾®åˆ†æœºåˆ¶ä¸æ¨¡å‹å®šä¹‰æ–¹æ³•

---

## ğŸ¯ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. ç¥ç»ç½‘ç»œåŸºç¡€

#### 1.1 æ„ŸçŸ¥æœº (Perceptron)

æ„ŸçŸ¥æœºæ˜¯æœ€ç®€å•çš„ç¥ç»ç½‘ç»œå•å…ƒï¼Œç”±ç¾å›½ç§‘å­¦å®¶Frank Rosenblattåœ¨1957å¹´æå‡ºã€‚

**æ•°å­¦è¡¨è¾¾å¼**:
```
y = f(w Â· x + b)
```
å…¶ä¸­:
- `x`: è¾“å…¥å‘é‡ (ç‰¹å¾)
- `w`: æƒé‡å‘é‡
- `b`: åç½® (bias)
- `f`: æ¿€æ´»å‡½æ•°
- `y`: è¾“å‡º

**é™åˆ¶**: æ„ŸçŸ¥æœºåªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜ï¼ˆå¦‚ANDã€ORï¼‰ï¼Œæ— æ³•è§£å†³XORé—®é¢˜ã€‚

#### 1.2 å¤šå±‚æ„ŸçŸ¥æœº (Multi-Layer Perceptron, MLP)

é€šè¿‡å †å å¤šä¸ªç¥ç»å…ƒå±‚ï¼ŒMLPå¯ä»¥è§£å†³éçº¿æ€§é—®é¢˜ã€‚

**ç½‘ç»œç»“æ„**:
```
è¾“å…¥å±‚ â†’ éšè—å±‚1 â†’ éšè—å±‚2 â†’ ... â†’ è¾“å‡ºå±‚
```

**å‰å‘ä¼ æ’­å…¬å¼** (Forward Propagation):
```
h1 = f1(W1 Â· x + b1)          # ç¬¬1å±‚
h2 = f2(W2 Â· h1 + b2)         # ç¬¬2å±‚
y = f_out(W_out Â· h2 + b_out) # è¾“å‡ºå±‚
```

#### 1.3 æ¿€æ´»å‡½æ•° (Activation Functions)

æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€¼è¿‘å¤æ‚å‡½æ•°ã€‚

| æ¿€æ´»å‡½æ•° | å…¬å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|------|---------|
| **Sigmoid** | Ïƒ(x) = 1 / (1 + e^(-x)) | è¾“å‡ºåœ¨(0,1)ï¼Œé€‚åˆæ¦‚ç‡ | æ¢¯åº¦æ¶ˆå¤±ã€è®¡ç®—æ…¢ | äºŒåˆ†ç±»è¾“å‡ºå±‚ |
| **Tanh** | tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x)) | è¾“å‡ºåœ¨(-1,1)ï¼Œé›¶ä¸­å¿ƒ | æ¢¯åº¦æ¶ˆå¤± | RNNéšè—å±‚ |
| **ReLU** | f(x) = max(0, x) | è®¡ç®—å¿«ã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤± | ç¥ç»å…ƒæ­»äº¡ | CNNéšè—å±‚ï¼ˆæœ€å¸¸ç”¨ï¼‰ |
| **Leaky ReLU** | f(x) = max(0.01x, x) | é¿å…ç¥ç»å…ƒæ­»äº¡ | éœ€è°ƒå‚ | æ·±åº¦ç½‘ç»œéšè—å±‚ |
| **GELU** | f(x) = x Â· Î¦(x) | æ€§èƒ½ä¼˜è¶Š | è®¡ç®—å¤æ‚ | Transformerï¼ˆBERT/GPTï¼‰ |
| **Softmax** | Ïƒ(x_i) = e^(x_i) / Î£e^(x_j) | è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ | ä¸é€‚åˆéšè—å±‚ | å¤šåˆ†ç±»è¾“å‡ºå±‚ |

**å¯è§†åŒ–å¯¹æ¯”**: å‚è§ `notebooks/stage4/01-neural-network.ipynb` ç¬¬2èŠ‚

---

### 2. åå‘ä¼ æ’­ç®—æ³• (Backpropagation)

åå‘ä¼ æ’­æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ã€‚

#### 2.1 æŸå¤±å‡½æ•° (Loss Function)

è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼çš„å·®è·ã€‚

**å¸¸ç”¨æŸå¤±å‡½æ•°**:

| ä»»åŠ¡ç±»å‹ | æŸå¤±å‡½æ•° | å…¬å¼ | PyTorchå®ç° |
|---------|---------|------|------------|
| äºŒåˆ†ç±» | äºŒå…ƒäº¤å‰ç†µ | BCE = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)] | `nn.BCELoss()` |
| å¤šåˆ†ç±» | äº¤å‰ç†µ | CE = -Î£ y_i Â· log(Å·_i) | `nn.CrossEntropyLoss()` |
| å›å½’ | å‡æ–¹è¯¯å·® | MSE = (1/n) Â· Î£(y - Å·)Â² | `nn.MSELoss()` |
| å›å½’ | å¹³å‡ç»å¯¹è¯¯å·® | MAE = (1/n) Â· Î£\|y - Å·\| | `nn.L1Loss()` |

#### 2.2 æ¢¯åº¦è®¡ç®—

**é“¾å¼æ³•åˆ™** (Chain Rule):
```
âˆ‚L/âˆ‚w = âˆ‚L/âˆ‚y Â· âˆ‚y/âˆ‚z Â· âˆ‚z/âˆ‚w
```

å…¶ä¸­:
- `L`: æŸå¤±å‡½æ•°
- `y`: è¾“å‡º
- `z`: çº¿æ€§ç»„åˆ (z = wÂ·x + b)
- `w`: æƒé‡

**æ‰‹åŠ¨æ¨å¯¼ç¤ºä¾‹** (å•å±‚ç½‘ç»œ):
```python
# å‰å‘ä¼ æ’­
z = w * x + b
y_pred = sigmoid(z)
loss = (y_true - y_pred) ** 2

# åå‘ä¼ æ’­
dL_dy = -2 * (y_true - y_pred)
dy_dz = sigmoid(z) * (1 - sigmoid(z))
dz_dw = x

# æ¢¯åº¦
dL_dw = dL_dy * dy_dz * dz_dw
```

**è®¡ç®—å›¾å¯è§†åŒ–**: å‚è§ `notebooks/stage4/01-neural-network.ipynb` ç¬¬3èŠ‚

---

### 3. ä¼˜åŒ–å™¨ (Optimizers)

ä¼˜åŒ–å™¨å†³å®šå¦‚ä½•ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°ã€‚

#### 3.1 æ¢¯åº¦ä¸‹é™ (Gradient Descent)

**å…¬å¼**:
```
w_new = w_old - learning_rate * âˆ‚L/âˆ‚w
```

**ä¸‰ç§å˜ä½“**:

| ç±»å‹ | æ¯æ¬¡è¿­ä»£ä½¿ç”¨æ•°æ®é‡ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------------------|------|------|
| æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD) | å…¨éƒ¨æ•°æ® | æ”¶æ•›ç¨³å®š | å¤§æ•°æ®é›†å¾ˆæ…¢ |
| éšæœºæ¢¯åº¦ä¸‹é™ (SGD) | 1ä¸ªæ ·æœ¬ | é€Ÿåº¦å¿«ã€å¯åœ¨çº¿å­¦ä¹  | æ”¶æ•›ä¸ç¨³å®š |
| å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch GD) | batch_sizeä¸ªæ ·æœ¬ | å¹³è¡¡é€Ÿåº¦ä¸ç¨³å®šæ€§ | éœ€è°ƒbatch_size |

#### 3.2 å¸¸ç”¨ä¼˜åŒ–å™¨å¯¹æ¯”

| ä¼˜åŒ–å™¨ | æ ¸å¿ƒæ€æƒ³ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èå­¦ä¹ ç‡ |
|-------|---------|------|------|----------|
| **SGD** | åŸºç¡€æ¢¯åº¦ä¸‹é™ | ç®€å•ã€å¯å¤ç° | éœ€ç²¾ç»†è°ƒå‚ | 0.01-0.1 |
| **SGD + Momentum** | ç´¯ç§¯å†å²æ¢¯åº¦ | åŠ é€Ÿæ”¶æ•›ã€å‡å°‘éœ‡è¡ | éœ€é¢å¤–å‚æ•° | 0.01-0.1 |
| **RMSprop** | è‡ªé€‚åº”å­¦ä¹ ç‡ | é€‚åˆRNN | éœ€è°ƒè¡°å‡ç‡ | 0.001 |
| **Adam** | Momentum + RMSprop | è‡ªé€‚åº”ã€é²æ£’æ€§å¼º | å¯èƒ½è¿‡æ‹Ÿåˆ | 0.001-0.0001 |
| **AdamW** | Adam + æƒé‡è¡°å‡ | æ”¹è¿›æ­£åˆ™åŒ– | - | 0.001-0.0001 |

**é€‰æ‹©å»ºè®®**:
- **é»˜è®¤é¦–é€‰**: Adam (lr=0.001)
- **è¿½æ±‚æœ€ä¼˜æ€§èƒ½**: SGD + Momentum (lr=0.01, momentum=0.9) + å­¦ä¹ ç‡è°ƒåº¦
- **Transformeræ¨¡å‹**: AdamW (lr=5e-5)

**å…¬å¼å¯¹æ¯”**: å‚è§ `notebooks/stage4/01-neural-network.ipynb` ç¬¬4èŠ‚

---

### 4. æ­£åˆ™åŒ–æŠ€æœ¯ (Regularization)

é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚

#### 4.1 æƒé‡è¡°å‡ (Weight Decay / L2æ­£åˆ™åŒ–)

**åŸç†**: åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„L2èŒƒæ•°æƒ©ç½šé¡¹ã€‚

**å…¬å¼**:
```
Loss_total = Loss_original + Î» Â· Î£wÂ²
```

**PyTorchå®ç°**:
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
```

#### 4.2 Dropout

**åŸç†**: è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œæµ‹è¯•æ—¶ä½¿ç”¨å…¨éƒ¨ç¥ç»å…ƒã€‚

**å…¸å‹é…ç½®**:
- å…¨è¿æ¥å±‚: dropout=0.5
- CNNå·ç§¯å±‚: dropout=0.2-0.3
- RNN/Transformer: dropout=0.1

**PyTorchå®ç°**:
```python
self.dropout = nn.Dropout(p=0.5)
x = self.dropout(x)
```

#### 4.3 Batch Normalization

**åŸç†**: å½’ä¸€åŒ–æ¯ä¸ªbatchçš„æ¿€æ´»å€¼ï¼ŒåŠ é€Ÿè®­ç»ƒã€‚

**å…¬å¼**:
```
y = Î³ Â· (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

**ä¼˜ç‚¹**:
- åŠ é€Ÿæ”¶æ•›ï¼ˆå¯ä½¿ç”¨æ›´å¤§å­¦ä¹ ç‡ï¼‰
- å‡è½»æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
- è½»å¾®æ­£åˆ™åŒ–æ•ˆæœ

**PyTorchå®ç°**:
```python
self.bn = nn.BatchNorm1d(num_features)
x = self.bn(x)
```

---

### 5. PyTorchåŸºç¡€

#### 5.1 å¼ é‡æ“ä½œ (Tensor Operations)

PyTorchä¸­çš„å¼ é‡ç±»ä¼¼äºNumPyæ•°ç»„ï¼Œä½†æ”¯æŒGPUåŠ é€Ÿå’Œè‡ªåŠ¨å¾®åˆ†ã€‚

**åˆ›å»ºå¼ é‡**:
```python
import torch

# ä»Pythonåˆ—è¡¨åˆ›å»º
x = torch.tensor([1, 2, 3])

# åˆ›å»ºç‰¹æ®Šå¼ é‡
zeros = torch.zeros(3, 4)       # å…¨0
ones = torch.ones(2, 3)         # å…¨1
rand = torch.rand(3, 3)         # å‡åŒ€åˆ†å¸ƒ[0,1)
randn = torch.randn(3, 3)       # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
```

**å¸¸ç”¨æ“ä½œ**:
```python
# å½¢çŠ¶æ“ä½œ
x = torch.randn(2, 3, 4)
print(x.shape)                  # torch.Size([2, 3, 4])
y = x.view(2, -1)              # reshape: (2, 12)
z = x.transpose(1, 2)          # è½¬ç½®: (2, 4, 3)

# æ•°å­¦è¿ç®—
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])
c = a + b                      # é€å…ƒç´ ç›¸åŠ 
d = torch.dot(a, b)            # ç‚¹ç§¯: 32
e = torch.matmul(A, B)         # çŸ©é˜µä¹˜æ³•

# GPUåŠ é€Ÿ
if torch.cuda.is_available():
    x_gpu = x.to('cuda')       # ç§»åŠ¨åˆ°GPU
    y_gpu = x_gpu * 2
    y_cpu = y_gpu.to('cpu')    # ç§»å›CPU
```

#### 5.2 è‡ªåŠ¨å¾®åˆ† (Autograd)

PyTorchçš„æ ¸å¿ƒç‰¹æ€§ï¼Œè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

**åŸºæœ¬ç”¨æ³•**:
```python
# éœ€è¦æ¢¯åº¦çš„å¼ é‡
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2 + 3 * x + 1

# åå‘ä¼ æ’­
y.backward()

# æŸ¥çœ‹æ¢¯åº¦
print(x.grad)  # dy/dx = 2x + 3 = 7.0
```

**æ¢¯åº¦ç´¯ç§¯**:
```python
x = torch.tensor([1.0], requires_grad=True)

# ç¬¬ä¸€æ¬¡å‰å‘+åå‘
y1 = x ** 2
y1.backward()
print(x.grad)  # 2.0

# ç¬¬äºŒæ¬¡ï¼ˆæ¢¯åº¦ä¼šç´¯ç§¯ï¼‰
y2 = x ** 3
y2.backward()
print(x.grad)  # 2.0 + 3.0 = 5.0

# æ¸…é›¶æ¢¯åº¦
x.grad.zero_()
```

#### 5.3 æ¨¡å‹å®šä¹‰

**ä¸¤ç§æ–¹å¼**:

**æ–¹å¼1: ä½¿ç”¨`nn.Module`ï¼ˆæ¨èï¼‰**:
```python
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# å®ä¾‹åŒ–æ¨¡å‹
model = SimpleNN(input_dim=10, hidden_dim=20, output_dim=2)
```

**æ–¹å¼2: ä½¿ç”¨`nn.Sequential`ï¼ˆç®€æ´ï¼‰**:
```python
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 2)
)
```

#### 5.4 è®­ç»ƒå¾ªç¯

**å®Œæ•´ç¤ºä¾‹**:
```python
import torch
import torch.nn as nn
import torch.optim as optim

# 1. å‡†å¤‡æ•°æ®
X_train = torch.randn(100, 10)
y_train = torch.randint(0, 2, (100,))

# 2. å®šä¹‰æ¨¡å‹
model = SimpleNN(10, 20, 2)

# 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. è®­ç»ƒå¾ªç¯
num_epochs = 100
for epoch in range(num_epochs):
    # å‰å‘ä¼ æ’­
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
    loss.backward()        # è®¡ç®—æ¢¯åº¦
    optimizer.step()       # æ›´æ–°å‚æ•°

    # æ‰“å°è¿›åº¦
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 5. è¯„ä¼°æ¨¡å‹
model.eval()
with torch.no_grad():
    test_outputs = model(X_test)
    _, predicted = torch.max(test_outputs, 1)
    accuracy = (predicted == y_test).sum().item() / len(y_test)
    print(f'Accuracy: {accuracy:.2%}')
```

---

## ğŸ”¬ å®è·µç¯èŠ‚

### Notebook 1: æ‰‹å†™ç¥ç»ç½‘ç»œ (`01-neural-network.ipynb`)

**å†…å®¹**:
1. ä»é›¶å®ç°æ„ŸçŸ¥æœºï¼ˆä¸ä½¿ç”¨PyTorchï¼‰
2. å®ç°MLPå¹¶åœ¨2Dæ•°æ®é›†ä¸Šå¯è§†åŒ–å†³ç­–è¾¹ç•Œ
3. æ‰‹åŠ¨æ¨å¯¼å¹¶å®ç°åå‘ä¼ æ’­
4. å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„æ•ˆæœ
5. å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹

**æ•°æ®é›†**: åˆæˆæ•°æ®ï¼ˆXORé—®é¢˜ã€èºæ—‹åˆ†ç±»ï¼‰

**é¢„æœŸè¾“å‡º**:
- å†³ç­–è¾¹ç•Œå¯è§†åŒ–å›¾
- æŸå¤±æ›²çº¿
- æ¢¯åº¦æµå¯è§†åŒ–

---

### Notebook 2: PyTorchåŸºç¡€ (`02-pytorch-basics.ipynb`)

**å†…å®¹**:
1. å¼ é‡æ“ä½œç»ƒä¹ ï¼ˆåˆ›å»ºã€ç´¢å¼•ã€åˆ‡ç‰‡ã€è¿ç®—ï¼‰
2. è‡ªåŠ¨å¾®åˆ†æœºåˆ¶æ¼”ç¤ºï¼ˆè®¡ç®—å›¾å¯è§†åŒ–ï¼‰
3. ä½¿ç”¨`nn.Module`å®šä¹‰æ¨¡å‹
4. åœ¨MNISTæ•°æ®é›†ä¸Šè®­ç»ƒæ‰‹å†™æ•°å­—åˆ†ç±»å™¨
5. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨çš„æ”¶æ•›é€Ÿåº¦
6. å®éªŒæ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆDropoutã€Weight Decayï¼‰

**æ•°æ®é›†**: MNISTï¼ˆæ‰‹å†™æ•°å­—0-9ï¼‰

**é¢„æœŸè¾“å‡º**:
- æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ° >95% å‡†ç¡®ç‡
- ä¸åŒä¼˜åŒ–å™¨çš„æŸå¤±æ›²çº¿å¯¹æ¯”å›¾
- æ­£åˆ™åŒ–æ•ˆæœå¯¹æ¯”ï¼ˆè¿‡æ‹Ÿåˆvsæ­£å¸¸åŒ–ï¼‰

---

## ğŸ“– æ¨èé˜…è¯»

### å¿…è¯»

- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Michael Nielsen (å…è´¹åœ¨çº¿ä¹¦)
- [Deep Learning Book Chapter 6: Deep Feedforward Networks](https://www.deeplearningbook.org/contents/mlp.html) - Goodfellow et al.
- [PyTorchå®˜æ–¹æ•™ç¨‹: 60åˆ†é’Ÿå…¥é—¨](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

### é€‰è¯»

- [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/) - Stanfordå¤§å­¦è¯¾ç¨‹ï¼ˆå¼ºçƒˆæ¨èï¼‰
- [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/) - çŸ©é˜µå¾®ç§¯åˆ†é€ŸæŸ¥
- [An Overview of Gradient Descent Optimization Algorithms](https://ruder.io/optimizing-gradient-descent/) - Sebastian Ruder

---

## â“ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä½¿ç”¨ReLUè€Œä¸æ˜¯Sigmoidï¼Ÿ

**A**: ReLUçš„ä¼˜åŠ¿ï¼š
1. **è®¡ç®—å¿«**: åªéœ€æ¯”è¾ƒå¤§å°ï¼Œæ— éœ€æŒ‡æ•°è¿ç®—
2. **ç¼“è§£æ¢¯åº¦æ¶ˆå¤±**: Sigmoidåœ¨è¾“å…¥è¾ƒå¤§/è¾ƒå°æ—¶æ¢¯åº¦æ¥è¿‘0ï¼ŒReLUåœ¨æ­£åŒºé—´æ¢¯åº¦æ’ä¸º1
3. **ç¨€ç–æ¿€æ´»**: çº¦50%çš„ç¥ç»å…ƒè¾“å‡ºä¸º0ï¼Œæé«˜æ•ˆç‡

ä½†ReLUä¹Ÿæœ‰**ç¥ç»å…ƒæ­»äº¡**é—®é¢˜ï¼ˆè´ŸåŒºé—´æ¢¯åº¦ä¸º0ï¼‰ï¼Œå¯ç”¨Leaky ReLUç¼“è§£ã€‚

---

### Q2: å¦‚ä½•é€‰æ‹©å­¦ä¹ ç‡ï¼Ÿ

**A**: å­¦ä¹ ç‡é€‰æ‹©ç­–ç•¥ï¼š
1. **ç»éªŒå€¼**: Adamä½¿ç”¨0.001ï¼ŒSGDä½¿ç”¨0.01-0.1
2. **å­¦ä¹ ç‡æŸ¥æ‰¾** (Learning Rate Finder): ä»å¾ˆå°å€¼å¼€å§‹ï¼Œé€æ­¥å¢å¤§ï¼Œç»˜åˆ¶lossæ›²çº¿ï¼Œé€‰æ‹©ä¸‹é™æœ€å¿«çš„ç‚¹
3. **å­¦ä¹ ç‡è°ƒåº¦** (Learning Rate Scheduling):
   - `StepLR`: æ¯Nä¸ªepoché™ä½å­¦ä¹ ç‡
   - `CosineAnnealingLR`: ä½™å¼¦é€€ç«
   - `ReduceLROnPlateau`: éªŒè¯é›†lossä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡

**PyTorchç¤ºä¾‹**:
```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

for epoch in range(100):
    train(...)
    scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
```

---

### Q3: è¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

**A**: å¸¸ç”¨æ–¹æ³•ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
1. **å¢åŠ æ•°æ®**: æœ€æœ‰æ•ˆï¼Œè€ƒè™‘æ•°æ®å¢å¼º (Data Augmentation)
2. **æ­£åˆ™åŒ–**:
   - L2æ­£åˆ™åŒ– (weight_decay=1e-4)
   - Dropout (p=0.5)
3. **ç®€åŒ–æ¨¡å‹**: å‡å°‘å±‚æ•°æˆ–ç¥ç»å…ƒæ•°é‡
4. **Early Stopping**: éªŒè¯é›†lossä¸ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ
5. **Batch Normalization**: è½»å¾®æ­£åˆ™åŒ–æ•ˆæœ

**è¯Šæ–­æ–¹æ³•**:
- ç»˜åˆ¶è®­ç»ƒé›†vséªŒè¯é›†çš„lossæ›²çº¿
- è®­ç»ƒlossæŒç»­ä¸‹é™ä½†éªŒè¯lossä¸Šå‡ â†’ è¿‡æ‹Ÿåˆ

---

### Q4: GPUåŠ é€Ÿå¦‚ä½•ä½¿ç”¨ï¼Ÿ

**A**: PyTorchä¸­ä½¿ç”¨GPUçš„æ­¥éª¤ï¼š

```python
# 1. æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# 2. å°†æ¨¡å‹ç§»åˆ°GPU
model = model.to(device)

# 3. å°†æ•°æ®ç§»åˆ°GPU
X_train = X_train.to(device)
y_train = y_train.to(device)

# 4. è®­ç»ƒï¼ˆå…¶ä»–ä»£ç ä¸å˜ï¼‰
outputs = model(X_train)  # è‡ªåŠ¨åœ¨GPUä¸Šè®¡ç®—
```

**æ³¨æ„äº‹é¡¹**:
- CPUå¼ é‡å’ŒGPUå¼ é‡ä¸èƒ½ç›´æ¥è¿ç®—
- ä½¿ç”¨`.to('cpu')`å°†GPUå¼ é‡ç§»å›CPUï¼ˆå¦‚ç»˜å›¾æ—¶ï¼‰

---

### Q5: æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å¦‚ä½•è§£å†³ï¼Ÿ

**A**:

**æ¢¯åº¦æ¶ˆå¤±** (Gradient Vanishing):
- ç—‡çŠ¶: è®­ç»ƒå¾ˆæ…¢ï¼Œlosså‡ ä¹ä¸ä¸‹é™
- åŸå› : Sigmoid/Tanhåœ¨æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦è¡°å‡åˆ°æ¥è¿‘0
- è§£å†³æ–¹æ¡ˆ:
  1. ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
  2. Batch Normalization
  3. æ®‹å·®è¿æ¥ (Residual Connection, ResNet)

**æ¢¯åº¦çˆ†ç‚¸** (Gradient Exploding):
- ç—‡çŠ¶: losså˜ä¸ºNaNæˆ–Inf
- åŸå› : æ¢¯åº¦åœ¨åå‘ä¼ æ’­ä¸­æŒ‡æ•°å¢é•¿
- è§£å†³æ–¹æ¡ˆ:
  1. æ¢¯åº¦è£å‰ª (Gradient Clipping):
     ```python
     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
     ```
  2. é™ä½å­¦ä¹ ç‡
  3. Batch Normalization

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å¯ä»¥:

1. ğŸ“˜ ç»§ç»­å­¦ä¹  **æ¨¡å—M02: è®¡ç®—æœºè§†è§‰åŸºç¡€** (`docs/stage4/02-cv-basics/README.md`)
2. ğŸ¯ é€‰æ‹©ä¸€ä¸ªæ·±åº¦å­¦ä¹ é¡¹ç›®å¼€å§‹å®è·µï¼ˆæ¨èP02: YOLOv11å®æ—¶æ£€æµ‹ï¼‰
3. ğŸ“š æ·±å…¥é˜…è¯» Deep Learning Book çš„å…¶ä»–ç« èŠ‚

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆä»¥ä¸‹ä»»åŠ¡è¡¨ç¤ºä½ å·²æŒæ¡æœ¬æ¨¡å—å†…å®¹ï¼š

- [ ] èƒ½è§£é‡Šæ„ŸçŸ¥æœºä¸MLPçš„åŒºåˆ«
- [ ] èƒ½æ‰‹å†™åå‘ä¼ æ’­çš„æ¢¯åº¦æ¨å¯¼ï¼ˆè‡³å°‘å•å±‚ï¼‰
- [ ] ç†è§£å¸¸ç”¨æ¿€æ´»å‡½æ•°çš„ä¼˜ç¼ºç‚¹åŠé€‚ç”¨åœºæ™¯
- [ ] èƒ½ä½¿ç”¨PyTorchå®šä¹‰ä¸€ä¸ªå¤šå±‚ç¥ç»ç½‘ç»œ
- [ ] èƒ½é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
- [ ] ç†è§£æ­£åˆ™åŒ–æŠ€æœ¯çš„åŸç†å¹¶èƒ½åº”ç”¨
- [ ] èƒ½è¯Šæ–­å¹¶è§£å†³è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆé—®é¢˜
- [ ] èƒ½ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ

---

**ä¸Šä¸€æ¨¡å—**: [é˜¶æ®µ3æ¨¡å—M04: æœºå™¨å­¦ä¹ è¿›é˜¶](../../stage3/04-ml-advanced/README.md)
**ä¸‹ä¸€æ¨¡å—**: [æ¨¡å—M02: è®¡ç®—æœºè§†è§‰åŸºç¡€](../02-cv-basics/README.md)
**è¿”å›**: [é˜¶æ®µ4ç›®å½•](../README.md)
