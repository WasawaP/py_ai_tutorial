# Pandasé¡¹ç›®å®æˆ˜ (Pandas Practice)

**æ¨¡å—ID**: M02
**æ‰€å±é˜¶æ®µ**: Stage 3 - æœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜
**é¢„è®¡å­¦ä¹ æ—¶é—´**: 1-1.5å°æ—¶
**éš¾åº¦**: â­â­ åˆçº§-ä¸­çº§

---

## ğŸ“– æ¨¡å—ç®€ä»‹

æœ¬æ¨¡å—æ˜¯M01ç§‘å­¦è®¡ç®—åº“çš„å®æˆ˜å»¶ä¼¸ï¼Œå°†å¸¦ä½ é€šè¿‡çœŸå®æ•°æ®åˆ†æé¡¹ç›®æ·±å…¥æŒæ¡Pandasçš„æ ¸å¿ƒæŠ€èƒ½ï¼š
- **æè¿°æ€§åˆ†æ**: ç†è§£æ•°æ®çš„é›†ä¸­è¶‹åŠ¿ã€ç¦»æ•£ç¨‹åº¦ã€åˆ†å¸ƒç‰¹å¾
- **æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)**: å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼ã€å¼‚å¸¸å€¼ã€å…³ç³»
- **æ•°æ®é¢„å¤„ç†**: æ•°æ®åˆå¹¶ã€æ¸…æ´—ã€æ ‡å‡†åŒ–ã€ç‰¹å¾å·¥ç¨‹

è¿™äº›æŠ€èƒ½æ˜¯æ‰€æœ‰æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ é¡¹ç›®çš„å¿…å¤‡åŸºç¡€ã€‚

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. âœ… è¿›è¡Œå…¨é¢çš„æè¿°æ€§ç»Ÿè®¡åˆ†æ
2. âœ… ä½¿ç”¨å¯è§†åŒ–è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ
3. âœ… è¯†åˆ«å’Œå¤„ç†æ•°æ®è´¨é‡é—®é¢˜ï¼ˆç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤å€¼ï¼‰
4. âœ… è¿›è¡Œæ•°æ®åˆå¹¶ä¸æ‹¼æ¥ï¼ˆmergeã€concatã€joinï¼‰
5. âœ… åº”ç”¨æ•°æ®æ ‡å‡†åŒ–ä¸å½’ä¸€åŒ–
6. âœ… ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ–‡æœ¬æ•°æ®æ¸…æ´—
7. âœ… è¿›è¡Œç‰¹å¾å·¥ç¨‹ï¼ˆç¼–ç ã€åˆ†ç®±ã€æ´¾ç”Ÿç‰¹å¾ï¼‰
8. âœ… ç‹¬ç«‹å®Œæˆç«¯åˆ°ç«¯çš„æ•°æ®åˆ†æé¡¹ç›®

---

## ğŸ“š çŸ¥è¯†ç‚¹æ¸…å•

### 1. æè¿°æ€§åˆ†æ

<details>
<summary><strong>æ ¸å¿ƒæ¦‚å¿µ</strong></summary>

**é›†ä¸­è¶‹åŠ¿**:
- å‡å€¼ (Mean): æ•°æ®çš„å¹³å‡æ°´å¹³
- ä¸­ä½æ•° (Median): æ•°æ®çš„ä¸­é—´å€¼ï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- ä¼—æ•° (Mode): å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼
- å‡ ä½•å¹³å‡æ•°: é€‚ç”¨äºå¢é•¿ç‡æ•°æ®

**ç¦»æ•£ç¨‹åº¦**:
- æ–¹å·® (Variance): æ•°æ®çš„åˆ†æ•£ç¨‹åº¦
- æ ‡å‡†å·® (Standard Deviation): æ–¹å·®çš„å¹³æ–¹æ ¹ï¼Œä¸æ•°æ®åŒå•ä½
- æå·® (Range): æœ€å¤§å€¼ - æœ€å°å€¼
- å››åˆ†ä½è· (IQR): Q3 - Q1ï¼Œè¡¡é‡ä¸­é—´50%æ•°æ®çš„åˆ†æ•£ç¨‹åº¦
- å˜å¼‚ç³»æ•° (CV): æ ‡å‡†å·®/å‡å€¼ï¼Œç”¨äºæ¯”è¾ƒä¸åŒé‡çº§æ•°æ®çš„ç¦»æ•£åº¦

**åˆ†å¸ƒå½¢æ€**:
- ååº¦ (Skewness): åˆ†å¸ƒçš„å¯¹ç§°æ€§
  - æ­£å: å³å°¾é•¿ï¼ˆmean > medianï¼‰
  - è´Ÿå: å·¦å°¾é•¿ï¼ˆmean < medianï¼‰
- å³°åº¦ (Kurtosis): åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
  - é«˜å³°: æ•°æ®é›†ä¸­åœ¨å‡å€¼é™„è¿‘
  - ä½å³°: æ•°æ®åˆ†æ•£

**Pandaså®ç°**:
```python
# åŸºç¡€ç»Ÿè®¡
df.describe()  # ä¸€æ¬¡æ€§è·å–æ‰€æœ‰ç»Ÿè®¡é‡

# å•ç‹¬è®¡ç®—
df['column'].mean()    # å‡å€¼
df['column'].median()  # ä¸­ä½æ•°
df['column'].mode()    # ä¼—æ•°
df['column'].std()     # æ ‡å‡†å·®
df['column'].var()     # æ–¹å·®
df['column'].quantile([0.25, 0.5, 0.75])  # åˆ†ä½æ•°

# é«˜çº§ç»Ÿè®¡
df['column'].skew()    # ååº¦
df['column'].kurt()    # å³°åº¦
```

**åº”ç”¨åœºæ™¯**:
- æ•°æ®è´¨é‡æ£€æŸ¥ï¼šå‘ç°å¼‚å¸¸å€¼
- ä¸šåŠ¡ç†è§£ï¼šäº†è§£æŒ‡æ ‡çš„å…¸å‹å€¼å’Œæ³¢åŠ¨èŒƒå›´
- ç‰¹å¾é€‰æ‹©ï¼šè¯†åˆ«æ–¹å·®è¿‡å°çš„æ— æ•ˆç‰¹å¾

</details>

---

### 2. æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)

<details>
<summary><strong>æ ¸å¿ƒæ¦‚å¿µ</strong></summary>

**EDAçš„ç›®æ ‡**:
1. ç†è§£æ•°æ®ç»“æ„å’Œç‰¹å¾
2. å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼å’Œå…³ç³»
3. è¯†åˆ«å¼‚å¸¸å€¼å’Œæ•°æ®è´¨é‡é—®é¢˜
4. å½¢æˆå»ºæ¨¡å‡è®¾

**EDAçš„æ­¥éª¤**:

**Step 1: æ•°æ®æ¦‚è§ˆ**
```python
# æŸ¥çœ‹æ•°æ®å½¢çŠ¶å’Œç±»å‹
print(df.shape)
print(df.dtypes)
print(df.info())

# æŸ¥çœ‹ç¼ºå¤±å€¼
print(df.isnull().sum())

# æŸ¥çœ‹å‰å‡ è¡Œ
print(df.head())
```

**Step 2: å•å˜é‡åˆ†æ**
```python
# æ•°å€¼å‹å˜é‡
df['numeric_col'].describe()
df['numeric_col'].hist(bins=50)  # åˆ†å¸ƒç›´æ–¹å›¾
df['numeric_col'].plot(kind='box')  # ç®±çº¿å›¾è¯†åˆ«å¼‚å¸¸å€¼

# ç±»åˆ«å‹å˜é‡
df['category_col'].value_counts()
df['category_col'].value_counts().plot(kind='bar')
```

**Step 3: åŒå˜é‡åˆ†æ**
```python
# æ•°å€¼ vs æ•°å€¼ï¼šæ•£ç‚¹å›¾
df.plot(x='var1', y='var2', kind='scatter')

# æ•°å€¼ vs ç±»åˆ«ï¼šåˆ†ç»„ç»Ÿè®¡
df.groupby('category')['numeric'].mean().plot(kind='bar')

# ç›¸å…³æ€§åˆ†æ
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True)
```

**Step 4: å¤šå˜é‡åˆ†æ**
```python
# æˆå¯¹å…³ç³»å›¾
sns.pairplot(df, hue='target')

# åˆ†ç»„ç®±çº¿å›¾
sns.boxplot(data=df, x='category', y='numeric', hue='target')
```

**å¼‚å¸¸å€¼è¯†åˆ«æ–¹æ³•**:
1. **IQRæ–¹æ³•**: è¶…å‡º [Q1-1.5*IQR, Q3+1.5*IQR] èŒƒå›´
2. **Z-scoreæ–¹æ³•**: |z| > 3 (å‡è®¾æ­£æ€åˆ†å¸ƒ)
3. **å¯è§†åŒ–**: ç®±çº¿å›¾ã€æ•£ç‚¹å›¾

```python
# IQRæ–¹æ³•
Q1 = df['column'].quantile(0.25)
Q3 = df['column'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
outliers = df[(df['column'] < lower_bound) | (df['column'] > upper_bound)]

# Z-scoreæ–¹æ³•
from scipy import stats
z_scores = np.abs(stats.zscore(df['column']))
outliers = df[z_scores > 3]
```

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
- EDAæ˜¯å»ºæ¨¡å‰çš„å¿…ç»æ­¥éª¤ï¼Œèƒ½é¿å…"åƒåœ¾è¿›ï¼Œåƒåœ¾å‡º"
- å¥½çš„EDAèƒ½èŠ‚çœå¤§é‡åç»­è°ƒè¯•æ—¶é—´
- å¾ˆå¤šæ•°æ®ç§‘å­¦ç«èµ›çš„å† å†›éƒ½å¼ºè°ƒEDAçš„é‡è¦æ€§

</details>

---

### 3. æ•°æ®åˆå¹¶ä¸æ‹¼æ¥

<details>
<summary><strong>æ ¸å¿ƒæ¦‚å¿µ</strong></summary>

**merge() - SQLé£æ ¼çš„è¿æ¥**:
```python
# å†…è¿æ¥ (INNER JOIN)
pd.merge(df1, df2, on='key', how='inner')

# å·¦è¿æ¥ (LEFT JOIN)
pd.merge(df1, df2, on='key', how='left')

# å³è¿æ¥ (RIGHT JOIN)
pd.merge(df1, df2, on='key', how='right')

# å…¨å¤–è¿æ¥ (FULL OUTER JOIN)
pd.merge(df1, df2, on='key', how='outer')

# å¤šé”®è¿æ¥
pd.merge(df1, df2, on=['key1', 'key2'])

# ä¸åŒåˆ—åè¿æ¥
pd.merge(df1, df2, left_on='id', right_on='user_id')
```

**concat() - æŒ‰è½´æ‹¼æ¥**:
```python
# å‚ç›´æ‹¼æ¥ï¼ˆå¢åŠ è¡Œï¼‰
pd.concat([df1, df2], axis=0, ignore_index=True)

# æ°´å¹³æ‹¼æ¥ï¼ˆå¢åŠ åˆ—ï¼‰
pd.concat([df1, df2], axis=1)

# ä¿ç•™æ¥æºæ ‡è®°
pd.concat([df1, df2], keys=['source1', 'source2'])
```

**join() - åŸºäºç´¢å¼•è¿æ¥**:
```python
df1.join(df2, how='left', lsuffix='_left', rsuffix='_right')
```

**é€‰æ‹©å“ªç§æ–¹æ³•ï¼Ÿ**
- `merge()`: åŸºäºåˆ—å€¼è¿æ¥ï¼ˆæœ€å¸¸ç”¨ï¼‰
- `concat()`: ç®€å•æ‹¼æ¥å¤šä¸ªDataFrame
- `join()`: åŸºäºç´¢å¼•è¿æ¥ï¼ˆè¾ƒå°‘ä½¿ç”¨ï¼‰

**å¸¸è§é—®é¢˜**:
- é‡å¤é”®å¯¼è‡´ç¬›å¡å°”ç§¯çˆ†ç‚¸
- è¿æ¥åæ•°æ®é‡å¼‚å¸¸ï¼ˆæ£€æŸ¥è¿æ¥é”®çš„å”¯ä¸€æ€§ï¼‰
- åˆ—åå†²çªï¼ˆä½¿ç”¨suffixeså‚æ•°ï¼‰

</details>

---

### 4. æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–

<details>
<summary><strong>æ ¸å¿ƒæ¦‚å¿µ</strong></summary>

**ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥**:
1. **åˆ é™¤**: æ•°æ®é‡å¤§ã€ç¼ºå¤±æ¯”ä¾‹å°æ—¶å¯è¡Œ
2. **å¡«å……**:
   - æ•°å€¼å‹: å‡å€¼ã€ä¸­ä½æ•°ã€ä¼—æ•°ã€æ’å€¼
   - ç±»åˆ«å‹: ä¼—æ•°ã€"Unknown"
   - æ—¶é—´åºåˆ—: å‰å‘å¡«å…… (ffill)ã€åå‘å¡«å…… (bfill)
3. **é¢„æµ‹**: ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼
4. **æ ‡è®°**: åˆ›å»º"æ˜¯å¦ç¼ºå¤±"çš„äºŒå€¼ç‰¹å¾

```python
# åˆ é™¤ç¼ºå¤±å€¼
df.dropna()  # åˆ é™¤ä»»æ„åˆ—æœ‰ç¼ºå¤±çš„è¡Œ
df.dropna(subset=['col1', 'col2'])  # åˆ é™¤æŒ‡å®šåˆ—æœ‰ç¼ºå¤±çš„è¡Œ
df.dropna(thresh=5)  # è‡³å°‘æœ‰5ä¸ªéç¼ºå¤±å€¼æ‰ä¿ç•™

# å¡«å……ç¼ºå¤±å€¼
df['col'].fillna(df['col'].mean())  # å‡å€¼å¡«å……
df['col'].fillna(df['col'].median())  # ä¸­ä½æ•°å¡«å……
df['col'].fillna(method='ffill')  # å‰å‘å¡«å……
df['col'].fillna(method='bfill')  # åå‘å¡«å……
df['col'].interpolate()  # æ’å€¼å¡«å……
```

**æ•°æ®æ ‡å‡†åŒ–ä¸å½’ä¸€åŒ–**:

**Min-Maxå½’ä¸€åŒ–** (ç¼©æ”¾åˆ°[0, 1]):
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['normalized'] = scaler.fit_transform(df[['column']])
# å…¬å¼: (x - min) / (max - min)
```

**Z-scoreæ ‡å‡†åŒ–** (å‡å€¼0ï¼Œæ ‡å‡†å·®1):
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['standardized'] = scaler.fit_transform(df[['column']])
# å…¬å¼: (x - mean) / std
```

**ä½•æ—¶ä½¿ç”¨ï¼Ÿ**
- Min-Max: æ•°æ®æœ‰æ˜ç¡®è¾¹ç•Œï¼Œä¸å—å¼‚å¸¸å€¼å½±å“æ—¶
- Z-score: æ•°æ®æ¥è¿‘æ­£æ€åˆ†å¸ƒï¼Œéœ€è¦ä¿ç•™åˆ†å¸ƒå½¢æ€æ—¶
- å†³ç­–æ ‘/éšæœºæ£®æ—: ä¸éœ€è¦æ ‡å‡†åŒ–
- çº¿æ€§æ¨¡å‹/ç¥ç»ç½‘ç»œ/KNN: éœ€è¦æ ‡å‡†åŒ–

**æ­£åˆ™è¡¨è¾¾å¼æ¸…æ´—æ–‡æœ¬**:
```python
import re

# æå–æ•°å­—
df['numeric'] = df['text_col'].str.extract(r'(\d+)', expand=False)

# æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
df['clean'] = df['text_col'].str.replace(r'[^\w\s]', '', regex=True)

# éªŒè¯æ ¼å¼ï¼ˆå¦‚é‚®ç®±ï¼‰
df['is_valid_email'] = df['email'].str.match(r'^[\w\.-]+@[\w\.-]+\.\w+$')

# åˆ†å‰²å­—ç¬¦ä¸²
df[['first', 'last']] = df['name'].str.split(' ', expand=True)
```

</details>

---

### 5. ç‰¹å¾å·¥ç¨‹

<details>
<summary><strong>æ ¸å¿ƒæ¦‚å¿µ</strong></summary>

**ç±»åˆ«ç‰¹å¾ç¼–ç **:

**1. Label Encoding** (åºå·ç¼–ç ):
```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['encoded'] = le.fit_transform(df['category'])
# é€‚ç”¨äºæœ‰åºç±»åˆ«ï¼šä½/ä¸­/é«˜ â†’ 0/1/2
```

**2. One-Hot Encoding** (ç‹¬çƒ­ç¼–ç ):
```python
pd.get_dummies(df, columns=['category'], drop_first=True)
# é€‚ç”¨äºæ— åºç±»åˆ«ï¼šçº¢/ç»¿/è“ â†’ [1,0,0], [0,1,0], [0,0,1]
```

**3. Frequency Encoding** (é¢‘ç‡ç¼–ç ):
```python
freq_map = df['category'].value_counts().to_dict()
df['frequency'] = df['category'].map(freq_map)
# é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼ˆå¦‚åŸå¸‚åï¼‰
```

**æ•°å€¼ç‰¹å¾åˆ†ç®±**:
```python
# ç­‰å®½åˆ†ç®±
df['age_bin'] = pd.cut(df['age'], bins=5, labels=['A', 'B', 'C', 'D', 'E'])

# ç­‰é¢‘åˆ†ç®±
df['age_qcut'] = pd.qcut(df['age'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])

# è‡ªå®šä¹‰åˆ†ç®±
bins = [0, 18, 35, 60, 100]
df['age_group'] = pd.cut(df['age'], bins=bins, labels=['å°‘å¹´', 'é’å¹´', 'ä¸­å¹´', 'è€å¹´'])
```

**æ´¾ç”Ÿç‰¹å¾**:
```python
# æ•°å­¦è¿ç®—
df['BMI'] = df['weight'] / (df['height'] / 100) ** 2

# æ—¶é—´ç‰¹å¾
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['dayofweek'] = df['date'].dt.dayofweek
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

# èšåˆç‰¹å¾
df['total_purchase'] = df.groupby('user_id')['amount'].transform('sum')
df['avg_purchase'] = df.groupby('user_id')['amount'].transform('mean')

# äº¤äº’ç‰¹å¾
df['price_per_sqm'] = df['price'] / df['area']
```

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
- "æ•°æ®å’Œç‰¹å¾å†³å®šäº†æœºå™¨å­¦ä¹ çš„ä¸Šé™ï¼Œè€Œæ¨¡å‹å’Œç®—æ³•åªæ˜¯é€¼è¿‘è¿™ä¸ªä¸Šé™è€Œå·²" - Andrew Ng
- å¥½çš„ç‰¹å¾å·¥ç¨‹èƒ½è®©ç®€å•æ¨¡å‹èƒœè¿‡å¤æ‚æ¨¡å‹

</details>

---

## ğŸ““ é…å¥—Notebook

æŒ‰é¡ºåºå­¦ä¹ ä»¥ä¸‹Notebookï¼š

| Notebook | ä¸»é¢˜ | æ—¶é•¿ | éš¾åº¦ |
|----------|------|------|------|
| [04-descriptive-analysis.ipynb](../../../notebooks/stage3/04-descriptive-analysis.ipynb) | æè¿°æ€§ç»Ÿè®¡ã€æ¢ç´¢æ€§åˆ†æã€å¼‚å¸¸å€¼æ£€æµ‹ | 25åˆ†é’Ÿ | â­â­ |
| [05-data-preprocessing.ipynb](../../../notebooks/stage3/05-data-preprocessing.ipynb) | æ•°æ®åˆå¹¶ã€æ¸…æ´—ã€æ ‡å‡†åŒ–ã€ç‰¹å¾å·¥ç¨‹ | 30åˆ†é’Ÿ | â­â­ |

**å­¦ä¹ å»ºè®®**:
1. å…ˆå®ŒæˆModule M01çš„3ä¸ªåŸºç¡€Notebook
2. æŒ‰é¡ºåºè¿è¡Œæ¯ä¸ªcellï¼Œç†è§£æ¯ä¸ªæ“ä½œçš„å«ä¹‰
3. å°è¯•ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿç»“æœå˜åŒ–
4. å®ŒæˆNotebookæœ«å°¾çš„ç»¼åˆç»ƒä¹ 
5. å¯é€‰ï¼šå°†å­¦åˆ°çš„æŠ€èƒ½åº”ç”¨åˆ°è‡ªå·±çš„æ•°æ®é›†

---

## ğŸ› ï¸ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1: ç”µå•†ç”¨æˆ·è¡Œä¸ºåˆ†æ

**æ•°æ®æè¿°**: åŒ…å«10ä¸‡æ¡ç”¨æˆ·è¡Œä¸ºè®°å½•ï¼ˆæµè§ˆã€æ”¶è—ã€åŠ è´­ã€è´­ä¹°ï¼‰

**ä»»åŠ¡**:
1. è¿›è¡Œå®Œæ•´çš„EDAï¼šæ•°æ®æ¦‚è§ˆã€ç¼ºå¤±å€¼ã€åˆ†å¸ƒã€ç›¸å…³æ€§
2. è®¡ç®—ç”¨æˆ·è¡Œä¸ºæ¼æ–—è½¬åŒ–ç‡ï¼ˆæµè§ˆâ†’æ”¶è—â†’åŠ è´­â†’è´­ä¹°ï¼‰
3. åˆ†æä¸åŒæ—¶é—´æ®µçš„ç”¨æˆ·æ´»è·ƒåº¦
4. è¯†åˆ«é«˜ä»·å€¼ç”¨æˆ·ï¼ˆRFMåˆ†æï¼‰

<details>
<summary><strong>æç¤º</strong></summary>

```python
import pandas as pd
import numpy as np

# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
n = 100000

data = {
    'user_id': np.random.randint(1000, 5000, n),
    'behavior': np.random.choice(['view', 'fav', 'cart', 'buy'], n, p=[0.6, 0.2, 0.15, 0.05]),
    'category': np.random.choice(['ç”µå­', 'æœè£…', 'é£Ÿå“', 'å›¾ä¹¦'], n),
    'timestamp': pd.date_range('2024-01-01', periods=n, freq='30s'),
    'price': np.random.exponential(100, n)
}
df = pd.DataFrame(data)

# 1. EDA
print(df.info())
print(df.describe())
print(df['behavior'].value_counts())

# 2. æ¼æ–—åˆ†æ
funnel = df['behavior'].value_counts()
print("\nè½¬åŒ–æ¼æ–—:")
print(f"æµè§ˆ: {funnel.get('view', 0)}")
print(f"æ”¶è—: {funnel.get('fav', 0)} ({funnel.get('fav', 0)/funnel.get('view', 1)*100:.2f}%)")
print(f"åŠ è´­: {funnel.get('cart', 0)} ({funnel.get('cart', 0)/funnel.get('view', 1)*100:.2f}%)")
print(f"è´­ä¹°: {funnel.get('buy', 0)} ({funnel.get('buy', 0)/funnel.get('view', 1)*100:.2f}%)")

# 3. æ—¶é—´åˆ†æ
df['hour'] = df['timestamp'].dt.hour
hourly = df.groupby('hour').size()
hourly.plot(kind='line', title='æ¯å°æ—¶æ´»è·ƒåº¦')

# 4. RFMåˆ†æï¼ˆä»…è´­ä¹°è¡Œä¸ºï¼‰
buy_df = df[df['behavior'] == 'buy'].copy()
now = df['timestamp'].max()

rfm = buy_df.groupby('user_id').agg({
    'timestamp': lambda x: (now - x.max()).days,  # Recency
    'user_id': 'count',  # Frequency
    'price': 'sum'  # Monetary
})
rfm.columns = ['Recency', 'Frequency', 'Monetary']

# åˆ†æ®µ
rfm['R_Score'] = pd.qcut(rfm['Recency'], 4, labels=[4, 3, 2, 1])
rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4])
rfm['M_Score'] = pd.qcut(rfm['Monetary'], 4, labels=[1, 2, 3, 4])
rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)

print("\nRFM Top 10 ç”¨æˆ·:")
print(rfm.sort_values('Monetary', ascending=False).head(10))
```

</details>

---

### ç»ƒä¹ 2: æˆ¿ä»·æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹

**æ•°æ®æè¿°**: æˆ¿ä»·æ•°æ®é›†ï¼ŒåŒ…å«ä½ç½®ã€é¢ç§¯ã€æˆ¿é¾„ã€é…ç½®ç­‰ç‰¹å¾ï¼Œå­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜

**ä»»åŠ¡**:
1. å¤„ç†ç¼ºå¤±å€¼ï¼ˆè‡³å°‘ä½¿ç”¨3ç§ä¸åŒç­–ç•¥ï¼‰
2. è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRå’ŒZ-scoreä¸¤ç§æ–¹æ³•ï¼‰
3. æ•°æ®æ ‡å‡†åŒ–ï¼ˆæ¯”è¾ƒMin-Maxå’ŒZ-scoreçš„æ•ˆæœï¼‰
4. ç‰¹å¾å·¥ç¨‹ï¼š
   - ç±»åˆ«ç‰¹å¾ç¼–ç ï¼ˆæœå‘ã€è£…ä¿®ç­‰ï¼‰
   - æ—¶é—´ç‰¹å¾æ´¾ç”Ÿï¼ˆå»ºé€ å¹´ä»½â†’æˆ¿é¾„ã€æˆ¿é¾„åˆ†æ®µï¼‰
   - äº¤äº’ç‰¹å¾ï¼ˆå•ä»·ã€æ€»ä»·/é¢ç§¯æ¯”ï¼‰
5. åˆ›å»ºæœ€ç»ˆçš„å»ºæ¨¡æ•°æ®é›†

<details>
<summary><strong>ç¤ºä¾‹æ•°æ®</strong></summary>

```python
# åˆ›å»ºæ¨¡æ‹Ÿæˆ¿ä»·æ•°æ®ï¼ˆå«æ•°æ®è´¨é‡é—®é¢˜ï¼‰
np.random.seed(42)
n = 500

data = {
    'price': np.random.normal(500, 150, n),  # ä¸‡å…ƒ
    'area': np.random.normal(100, 30, n),  # å¹³ç±³
    'rooms': np.random.choice([1, 2, 3, 4], n, p=[0.1, 0.3, 0.4, 0.2]),
    'build_year': np.random.randint(1990, 2024, n),
    'floor': np.random.randint(1, 33, n),
    'direction': np.random.choice(['å—', 'åŒ—', 'ä¸œ', 'è¥¿', 'å—åŒ—'], n),
    'decoration': np.random.choice(['æ¯›å¯', 'ç®€è£…', 'ç²¾è£…', 'è±ªè£…'], n),
    'district': np.random.choice(['æœé˜³', 'æµ·æ·€', 'è¥¿åŸ', 'ä¸œåŸ', 'ä¸°å°'], n)
}
df = pd.DataFrame(data)

# å¼•å…¥ç¼ºå¤±å€¼
df.loc[np.random.choice(df.index, 50), 'price'] = np.nan
df.loc[np.random.choice(df.index, 30), 'area'] = np.nan
df.loc[np.random.choice(df.index, 20), 'build_year'] = np.nan

# å¼•å…¥å¼‚å¸¸å€¼
df.loc[np.random.choice(df.index, 5), 'price'] = np.random.uniform(2000, 5000, 5)  # è¶…é«˜æˆ¿ä»·
df.loc[np.random.choice(df.index, 3), 'area'] = np.random.uniform(500, 1000, 3)  # è¶…å¤§é¢ç§¯

print("åŸå§‹æ•°æ®å½¢çŠ¶:", df.shape)
print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(df.isnull().sum())
print("\næ•°æ®é¢„è§ˆ:")
print(df.head())
```

</details>

---

## ğŸ“ è¿›é˜¶æ‹“å±•

å®ŒæˆåŸºç¡€å­¦ä¹ åï¼Œå¯é€‰æ‹©æ·±å…¥ä»¥ä¸‹ä¸»é¢˜ï¼š

1. **æ—¶é—´åºåˆ—åˆ†æ**
   - ç§»åŠ¨å¹³å‡ã€æŒ‡æ•°å¹³æ»‘
   - å­£èŠ‚æ€§åˆ†è§£
   - ARIMAæ¨¡å‹

2. **æ–‡æœ¬æ•°æ®å¤„ç†**
   - åˆ†è¯ã€è¯é¢‘ç»Ÿè®¡
   - TF-IDFå‘é‡åŒ–
   - æƒ…æ„Ÿåˆ†æ

3. **å¤§æ•°æ®å¤„ç†**
   - Dask: Pandasçš„åˆ†å¸ƒå¼ç‰ˆæœ¬
   - Polars: æ›´å¿«çš„DataFrameåº“
   - Apache Spark: å¤§è§„æ¨¡æ•°æ®å¤„ç†

4. **è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹**
   - Featuretools: è‡ªåŠ¨ç”Ÿæˆç‰¹å¾
   - TPOT/AutoGluon: è‡ªåŠ¨æœºå™¨å­¦ä¹ 

---

## ğŸ“– æ¨èèµ„æº

### ä¹¦ç±
- ã€ŠPythonæ•°æ®åˆ†æå®æˆ˜ã€‹- Fabio Nelli
- ã€ŠFeature Engineering for Machine Learningã€‹- Alice Zheng

### è¯¾ç¨‹
- Kaggle Learn: [Pandas](https://www.kaggle.com/learn/pandas)
- Kaggle Learn: [Data Cleaning](https://www.kaggle.com/learn/data-cleaning)
- Kaggle Learn: [Feature Engineering](https://www.kaggle.com/learn/feature-engineering)

### å®æˆ˜é¡¹ç›®
- Kaggleç«èµ›: [Titanic](https://www.kaggle.com/c/titanic) (å…¥é—¨çº§)
- Kaggleç«èµ›: [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)

---

## âœ… è‡ªæµ‹æ¸…å•

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] è®¡ç®—å¹¶è§£é‡Šæè¿°æ€§ç»Ÿè®¡é‡ï¼ˆå‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ã€ååº¦ã€å³°åº¦ï¼‰
- [ ] è¿›è¡Œå®Œæ•´çš„EDAæµç¨‹ï¼ˆå•å˜é‡ã€åŒå˜é‡ã€å¤šå˜é‡åˆ†æï¼‰
- [ ] è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸å€¼ï¼ˆIQRã€Z-scoreæ–¹æ³•ï¼‰
- [ ] ä½¿ç”¨merge/concatåˆå¹¶å¤šä¸ªæ•°æ®æº
- [ ] å¤„ç†ç¼ºå¤±å€¼ï¼ˆåˆ é™¤ã€å¡«å……ã€æ ‡è®°ï¼‰
- [ ] åº”ç”¨æ•°æ®æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–
- [ ] ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…æ´—æ–‡æœ¬æ•°æ®
- [ ] è¿›è¡Œç±»åˆ«ç‰¹å¾ç¼–ç ï¼ˆLabel Encodingã€One-Hot Encodingï¼‰
- [ ] åˆ›å»ºæ´¾ç”Ÿç‰¹å¾å’Œäº¤äº’ç‰¹å¾
- [ ] ç‹¬ç«‹å®Œæˆç«¯åˆ°ç«¯çš„æ•°æ®é¢„å¤„ç†æµç¨‹

**é€šè¿‡æ ‡å‡†**: å®Œæˆ2ä¸ªå®æˆ˜ç»ƒä¹ ï¼Œèƒ½æ¸…æ™°è§£é‡Šæ¯ä¸ªæ•°æ®å¤„ç†æ­¥éª¤çš„ç›®çš„

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å¯ä»¥ï¼š

1. **ç»§ç»­å­¦ä¹ æ¨¡å—M03**: [AIæ•°å­¦åŸºç¡€](../03-ml-basics/README.md) - ç»Ÿè®¡å­¦ã€çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºé€Ÿè§ˆ
2. **å¼€å§‹é¡¹ç›®å®æˆ˜**: [é¡¹ç›®P01 - æœé˜³åŒ»é™¢æ•°æ®åˆ†æ](../projects/p01-healthcare/README.md)
3. **æ·±å…¥ç‰¹å¾å·¥ç¨‹**: å­¦ä¹ æ›´é«˜çº§çš„ç‰¹å¾æ„å»ºæŠ€å·§

**é‡è¦**: Pandasæ•°æ®å¤„ç†èƒ½åŠ›æ˜¯æ•°æ®ç§‘å­¦å®¶çš„æ ¸å¿ƒç«äº‰åŠ›ï¼Œå¤šç»ƒä¹ ã€å¤šå®è·µï¼

---

## ğŸ’¬ è®¨è®ºä¸åé¦ˆ

é‡åˆ°é—®é¢˜ï¼Ÿæœ‰æ”¹è¿›å»ºè®®ï¼Ÿ

- ğŸ’¬ åŠ å…¥å­¦ä¹ ç¤¾ç¾¤è®¨è®º
- ğŸ› æäº¤[GitHub Issue](https://github.com/shychee/py_ai_tutorial/issues)
- ğŸ“§ å‘é€é‚®ä»¶è‡³ shychee96@gmail.com

**æŒæ¡æ•°æ®æ¸…æ´—ï¼Œä½ å°±æŒæ¡äº†æ•°æ®ç§‘å­¦çš„80%ï¼** ğŸ’ª
