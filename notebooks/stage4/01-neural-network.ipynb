{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ‰‹å†™ç¥ç»ç½‘ç»œï¼šä»æ„ŸçŸ¥æœºåˆ°å¤šå±‚ç½‘ç»œ\n",
    "\n",
    "**ç›®æ ‡**: ä»é›¶å®ç°ç¥ç»ç½‘ç»œï¼Œæ·±å…¥ç†è§£åå‘ä¼ æ’­ç®—æ³•\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**: 60-90åˆ†é’Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ æœ¬Notebookå†…å®¹\n",
    "\n",
    "1. ä»é›¶å®ç°æ„ŸçŸ¥æœºï¼ˆä¸ä½¿ç”¨æ¡†æ¶ï¼‰\n",
    "2. å®ç°å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰\n",
    "3. å¯è§†åŒ–æ¿€æ´»å‡½æ•°\n",
    "4. æ‰‹åŠ¨æ¨å¯¼å¹¶å®ç°åå‘ä¼ æ’­\n",
    "5. åœ¨XORé—®é¢˜ä¸Šè®­ç»ƒå¹¶å¯è§†åŒ–å†³ç­–è¾¹ç•Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']  # macOS/Windows\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬1éƒ¨åˆ†: æ„ŸçŸ¥æœºå®ç°\n",
    "\n",
    "æ„ŸçŸ¥æœºæ˜¯æœ€ç®€å•çš„ç¥ç»ç½‘ç»œå•å…ƒï¼Œåªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"å•å±‚æ„ŸçŸ¥æœºå®ç°\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, learning_rate=0.01, epochs=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦\n",
    "            learning_rate: å­¦ä¹ ç‡\n",
    "            epochs: è®­ç»ƒè½®æ•°\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros(input_dim)\n",
    "        self.bias = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.losses = []\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"é˜¶è·ƒæ¿€æ´»å‡½æ•°: 0æˆ–1\"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"é¢„æµ‹\"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(z)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"è®­ç»ƒæ„ŸçŸ¥æœº\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            errors = 0\n",
    "            for xi, yi in zip(X, y):\n",
    "                # é¢„æµ‹\n",
    "                prediction = self.predict(xi)\n",
    "                \n",
    "                # æ›´æ–°æƒé‡ï¼ˆä»…å½“é¢„æµ‹é”™è¯¯æ—¶ï¼‰\n",
    "                error = yi - prediction\n",
    "                if error != 0:\n",
    "                    self.weights += self.learning_rate * error * xi\n",
    "                    self.bias += self.learning_rate * error\n",
    "                    errors += 1\n",
    "            \n",
    "            # è®°å½•æŸå¤±ï¼ˆé”™è¯¯åˆ†ç±»æ•°ï¼‰\n",
    "            self.losses.append(errors)\n",
    "            \n",
    "            # æ”¶æ•›æ£€æŸ¥\n",
    "            if errors == 0:\n",
    "                print(f\"æ”¶æ•›äºepoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æµ‹è¯•æ„ŸçŸ¥æœºï¼šANDé—¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANDé—¨æ•°æ®\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "# è®­ç»ƒæ„ŸçŸ¥æœº\n",
    "perceptron_and = Perceptron(input_dim=2, learning_rate=0.1, epochs=100)\n",
    "perceptron_and.fit(X_and, y_and)\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"\\nANDé—¨æµ‹è¯•ç»“æœ:\")\n",
    "for xi, yi in zip(X_and, y_and):\n",
    "    pred = perceptron_and.predict(xi)\n",
    "    print(f\"è¾“å…¥: {xi}, çœŸå®: {yi}, é¢„æµ‹: {pred}\")\n",
    "\n",
    "# ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(perceptron_and.losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('é”™è¯¯åˆ†ç±»æ•°')\n",
    "plt.title('ANDé—¨æ„ŸçŸ¥æœºè®­ç»ƒè¿‡ç¨‹')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ„ŸçŸ¥æœºçš„å±€é™ï¼šXORé—®é¢˜\n",
    "\n",
    "XORï¼ˆå¼‚æˆ–ï¼‰æ˜¯çº¿æ€§ä¸å¯åˆ†é—®é¢˜ï¼Œå•å±‚æ„ŸçŸ¥æœºæ— æ³•è§£å†³ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XORæ•°æ®\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# å°è¯•è®­ç»ƒæ„ŸçŸ¥æœº\n",
    "perceptron_xor = Perceptron(input_dim=2, learning_rate=0.1, epochs=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# æµ‹è¯•ï¼ˆä¼šå¤±è´¥ï¼‰\n",
    "print(\"\\nXORé—¨æµ‹è¯•ç»“æœ:\")\n",
    "for xi, yi in zip(X_xor, y_xor):\n",
    "    pred = perceptron_xor.predict(xi)\n",
    "    print(f\"è¾“å…¥: {xi}, çœŸå®: {yi}, é¢„æµ‹: {pred}, {'âœ“' if pred==yi else 'âœ—'}\")\n",
    "\n",
    "# å¯è§†åŒ–XORé—®é¢˜\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=100, label='ç±»åˆ«0', edgecolors='k')\n",
    "plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=100, label='ç±»åˆ«1', edgecolors='k')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('XORé—®é¢˜ï¼šçº¿æ€§ä¸å¯åˆ†')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nç»“è®º: å•å±‚æ„ŸçŸ¥æœºæ— æ³•è§£å†³XORé—®é¢˜ï¼Œéœ€è¦å¤šå±‚ç½‘ç»œï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬2éƒ¨åˆ†: æ¿€æ´»å‡½æ•°å¯è§†åŒ–\n",
    "\n",
    "æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œä½¿ç¥ç»ç½‘ç»œèƒ½å¤Ÿé€¼è¿‘å¤æ‚å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æ¿€æ´»å‡½æ•°\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# å®šä¹‰å¯¼æ•°ï¼ˆç”¨äºåç»­åå‘ä¼ æ’­ï¼‰\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# ç»˜åˆ¶æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(x, sigmoid(x), 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid')\n",
    "axes[0, 0].grid(True)\n",
    "axes[1, 0].plot(x, sigmoid_derivative(x), 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('Sigmoidå¯¼æ•°')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Tanh\n",
    "axes[0, 1].plot(x, tanh(x), 'b-', linewidth=2)\n",
    "axes[0, 1].set_title('Tanh')\n",
    "axes[0, 1].grid(True)\n",
    "axes[1, 1].plot(x, tanh_derivative(x), 'r-', linewidth=2)\n",
    "axes[1, 1].set_title('Tanhå¯¼æ•°')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 2].plot(x, relu(x), 'b-', linewidth=2)\n",
    "axes[0, 2].set_title('ReLU')\n",
    "axes[0, 2].grid(True)\n",
    "axes[1, 2].plot(x, relu_derivative(x), 'r-', linewidth=2)\n",
    "axes[1, 2].set_title('ReLUå¯¼æ•°')\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "# Leaky ReLU\n",
    "axes[0, 3].plot(x, leaky_relu(x), 'b-', linewidth=2)\n",
    "axes[0, 3].set_title('Leaky ReLU')\n",
    "axes[0, 3].grid(True)\n",
    "axes[1, 3].plot(x, np.where(x > 0, 1, 0.01), 'r-', linewidth=2)\n",
    "axes[1, 3].set_title('Leaky ReLUå¯¼æ•°')\n",
    "axes[1, 3].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è§‚å¯Ÿ**:\n",
    "- **Sigmoid**: è¾“å‡ºåœ¨(0,1)ï¼Œä½†åœ¨|x|>3æ—¶æ¢¯åº¦æ¥è¿‘0ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰\n",
    "- **Tanh**: è¾“å‡ºåœ¨(-1,1)ï¼Œæ¯”Sigmoidå¥½ï¼Œä½†ä»æœ‰æ¢¯åº¦æ¶ˆå¤±\n",
    "- **ReLU**: æ­£åŒºé—´æ¢¯åº¦æ’ä¸º1ï¼Œè´ŸåŒºé—´æ¢¯åº¦ä¸º0ï¼ˆç¥ç»å…ƒæ­»äº¡ï¼‰\n",
    "- **Leaky ReLU**: è´ŸåŒºé—´ä¿ç•™å°æ¢¯åº¦ï¼Œç¼“è§£ç¥ç»å…ƒæ­»äº¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬3éƒ¨åˆ†: å¤šå±‚æ„ŸçŸ¥æœº (MLP) å®ç°\n",
    "\n",
    "ä½¿ç”¨MLPè§£å†³XORé—®é¢˜ï¼Œæ‰‹åŠ¨å®ç°åå‘ä¼ æ’­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"ä¸¤å±‚ç¥ç»ç½‘ç»œå®ç°ï¼ˆ1ä¸ªéšè—å±‚ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: è¾“å…¥å±‚ç¥ç»å…ƒæ•°\n",
    "            hidden_dim: éšè—å±‚ç¥ç»å…ƒæ•°\n",
    "            output_dim: è¾“å‡ºå±‚ç¥ç»å…ƒæ•°\n",
    "            learning_rate: å­¦ä¹ ç‡\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–æƒé‡ï¼ˆXavieråˆå§‹åŒ–ï¼‰\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # ç”¨äºå­˜å‚¨ä¸­é—´å€¼ï¼ˆåå‘ä¼ æ’­éœ€è¦ï¼‰\n",
    "        self.cache = {}\n",
    "        self.losses = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"å‰å‘ä¼ æ’­\"\"\"\n",
    "        # éšè—å±‚\n",
    "        self.cache['z1'] = np.dot(X, self.W1) + self.b1\n",
    "        self.cache['a1'] = sigmoid(self.cache['z1'])\n",
    "        \n",
    "        # è¾“å‡ºå±‚\n",
    "        self.cache['z2'] = np.dot(self.cache['a1'], self.W2) + self.b2\n",
    "        self.cache['a2'] = sigmoid(self.cache['z2'])\n",
    "        \n",
    "        return self.cache['a2']\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        \"\"\"åå‘ä¼ æ’­ï¼ˆæ‰‹åŠ¨æ¨å¯¼ï¼‰\"\"\"\n",
    "        m = X.shape[0]  # æ ·æœ¬æ•°\n",
    "        \n",
    "        # è¾“å‡ºå±‚æ¢¯åº¦\n",
    "        # dL/dz2 = (y_pred - y) * sigmoid'(z2)\n",
    "        dz2 = y_pred - y\n",
    "        dW2 = np.dot(self.cache['a1'].T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "        \n",
    "        # éšè—å±‚æ¢¯åº¦\n",
    "        # dL/dz1 = dL/dz2 * W2 * sigmoid'(z1)\n",
    "        dz1 = np.dot(dz2, self.W2.T) * sigmoid_derivative(self.cache['z1'])\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "        \n",
    "        # æ›´æ–°å‚æ•°\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y, y_pred):\n",
    "        \"\"\"è®¡ç®—äºŒå…ƒäº¤å‰ç†µæŸå¤±\"\"\"\n",
    "        m = y.shape[0]\n",
    "        epsilon = 1e-8  # é˜²æ­¢log(0)\n",
    "        loss = -np.sum(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon)) / m\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=10000, verbose=True):\n",
    "        \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "        y = y.reshape(-1, 1)  # ç¡®ä¿yçš„å½¢çŠ¶æ­£ç¡®\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # å‰å‘ä¼ æ’­\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤±\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            self.backward(X, y, y_pred)\n",
    "            \n",
    "            # æ‰“å°è¿›åº¦\n",
    "            if verbose and (epoch + 1) % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"é¢„æµ‹\"\"\"\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åœ¨XORé—®é¢˜ä¸Šè®­ç»ƒMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¹¶è®­ç»ƒMLP\n",
    "mlp = MLP(input_dim=2, hidden_dim=4, output_dim=1, learning_rate=0.5)\n",
    "mlp.fit(X_xor, y_xor, epochs=5000, verbose=True)\n",
    "\n",
    "# æµ‹è¯•\n",
    "print(\"\\n=== MLPåœ¨XORé—®é¢˜ä¸Šçš„æµ‹è¯•ç»“æœ ===\")\n",
    "predictions = mlp.predict(X_xor)\n",
    "for xi, yi, pred in zip(X_xor, y_xor, predictions):\n",
    "    print(f\"è¾“å…¥: {xi}, çœŸå®: {yi}, é¢„æµ‹: {pred[0]}, {'âœ“' if pred[0]==yi else 'âœ—'}\")\n",
    "\n",
    "# è®¡ç®—å‡†ç¡®ç‡\n",
    "accuracy = np.mean(predictions.flatten() == y_xor)\n",
    "print(f\"\\nå‡†ç¡®ç‡: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mlp.losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('æŸå¤± (Binary Cross-Entropy)')\n",
    "plt.title('MLPè®­ç»ƒè¿‡ç¨‹')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mlp.losses[100:])  # è·³è¿‡å‰100ä¸ªepoch\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('æŸå¤±')\n",
    "plt.title('MLPè®­ç»ƒè¿‡ç¨‹ï¼ˆæ”¾å¤§ï¼‰')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¯è§†åŒ–å†³ç­–è¾¹ç•Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"å†³ç­–è¾¹ç•Œ\"):\n",
    "    \"\"\"ç»˜åˆ¶å†³ç­–è¾¹ç•Œ\"\"\"\n",
    "    # åˆ›å»ºç½‘æ ¼\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # é¢„æµ‹æ¯ä¸ªç½‘æ ¼ç‚¹\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\n",
    "    plt.contour(xx, yy, Z, colors='k', linewidths=1, levels=[0.5])\n",
    "    \n",
    "    # ç»˜åˆ¶æ•°æ®ç‚¹\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='red', s=200, label='ç±»åˆ«0', edgecolors='k', linewidths=2)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', s=200, label='ç±»åˆ«1', edgecolors='k', linewidths=2)\n",
    "    \n",
    "    plt.xlabel('x1', fontsize=12)\n",
    "    plt.ylabel('x2', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# ç»˜åˆ¶XORé—®é¢˜çš„å†³ç­–è¾¹ç•Œ\n",
    "plot_decision_boundary(mlp, X_xor, y_xor, title=\"MLPåœ¨XORé—®é¢˜ä¸Šçš„å†³ç­–è¾¹ç•Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è§‚å¯Ÿ**: MLPæˆåŠŸå­¦ä¹ åˆ°äº†éçº¿æ€§å†³ç­–è¾¹ç•Œï¼Œå°†XORé—®é¢˜çš„ä¸¤ç±»åˆ†å¼€ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬4éƒ¨åˆ†: åœ¨èºæ—‹æ•°æ®é›†ä¸Šæµ‹è¯•\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„éçº¿æ€§æ•°æ®é›†ï¼Œæµ‹è¯•MLPçš„èƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spiral_data(n_samples=100, n_classes=2, noise=0.2):\n",
    "    \"\"\"ç”Ÿæˆèºæ—‹æ•°æ®é›†\"\"\"\n",
    "    X = np.zeros((n_samples * n_classes, 2))\n",
    "    y = np.zeros(n_samples * n_classes, dtype=int)\n",
    "    \n",
    "    for j in range(n_classes):\n",
    "        ix = range(n_samples * j, n_samples * (j + 1))\n",
    "        r = np.linspace(0.0, 1, n_samples)\n",
    "        t = np.linspace(j * 4, (j + 1) * 4, n_samples) + np.random.randn(n_samples) * noise\n",
    "        X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n",
    "        y[ix] = j\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# ç”Ÿæˆèºæ—‹æ•°æ®\n",
    "X_spiral, y_spiral = make_spiral_data(n_samples=100, n_classes=2, noise=0.15)\n",
    "\n",
    "# å¯è§†åŒ–æ•°æ®\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_spiral[y_spiral==0, 0], X_spiral[y_spiral==0, 1], c='red', s=50, label='ç±»åˆ«0')\n",
    "plt.scatter(X_spiral[y_spiral==1, 0], X_spiral[y_spiral==1, 1], c='blue', s=50, label='ç±»åˆ«1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('èºæ—‹æ•°æ®é›†')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒMLP\n",
    "mlp_spiral = MLP(input_dim=2, hidden_dim=20, output_dim=1, learning_rate=0.5)\n",
    "mlp_spiral.fit(X_spiral, y_spiral, epochs=10000, verbose=True)\n",
    "\n",
    "# è¯„ä¼°\n",
    "predictions_spiral = mlp_spiral.predict(X_spiral)\n",
    "accuracy_spiral = np.mean(predictions_spiral.flatten() == y_spiral)\n",
    "print(f\"\\nèºæ—‹æ•°æ®é›†å‡†ç¡®ç‡: {accuracy_spiral:.2%}\")\n",
    "\n",
    "# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ\n",
    "plot_decision_boundary(mlp_spiral, X_spiral, y_spiral, title=\"MLPåœ¨èºæ—‹æ•°æ®é›†ä¸Šçš„å†³ç­–è¾¹ç•Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬5éƒ¨åˆ†: å®éªŒä¸åŒæ¿€æ´»å‡½æ•°\n",
    "\n",
    "å¯¹æ¯”Sigmoidã€Tanhã€ReLUåœ¨ç›¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿®æ”¹MLPç±»ä»¥æ”¯æŒä¸åŒæ¿€æ´»å‡½æ•°\n",
    "class MLPWithActivation:\n",
    "    \"\"\"æ”¯æŒä¸åŒæ¿€æ´»å‡½æ•°çš„MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.1, activation='sigmoid'):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.cache = {}\n",
    "        self.losses = []\n",
    "    \n",
    "    def activate(self, z):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid(z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return tanh(z)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu(z)\n",
    "    \n",
    "    def activate_derivative(self, z):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return sigmoid_derivative(z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return tanh_derivative(z)\n",
    "        elif self.activation == 'relu':\n",
    "            return relu_derivative(z)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache['z1'] = np.dot(X, self.W1) + self.b1\n",
    "        self.cache['a1'] = self.activate(self.cache['z1'])\n",
    "        self.cache['z2'] = np.dot(self.cache['a1'], self.W2) + self.b2\n",
    "        self.cache['a2'] = sigmoid(self.cache['z2'])  # è¾“å‡ºå±‚å§‹ç»ˆç”¨sigmoid\n",
    "        return self.cache['a2']\n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        m = X.shape[0]\n",
    "        dz2 = y_pred - y\n",
    "        dW2 = np.dot(self.cache['a1'].T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "        \n",
    "        dz1 = np.dot(dz2, self.W2.T) * self.activate_derivative(self.cache['z1'])\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "        \n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y, y_pred):\n",
    "        m = y.shape[0]\n",
    "        epsilon = 1e-8\n",
    "        loss = -np.sum(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon)) / m\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=5000, verbose=False):\n",
    "        y = y.reshape(-1, 1)\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            self.losses.append(loss)\n",
    "            self.backward(X, y, y_pred)\n",
    "            if verbose and (epoch + 1) % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "\n",
    "# å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "results = {}\n",
    "\n",
    "for act in activations:\n",
    "    print(f\"\\n=== è®­ç»ƒä½¿ç”¨{act}æ¿€æ´»å‡½æ•°çš„MLP ===\")\n",
    "    model = MLPWithActivation(input_dim=2, hidden_dim=10, output_dim=1, \n",
    "                              learning_rate=0.5, activation=act)\n",
    "    model.fit(X_xor, y_xor, epochs=5000, verbose=False)\n",
    "    \n",
    "    predictions = model.predict(X_xor)\n",
    "    accuracy = np.mean(predictions.flatten() == y_xor)\n",
    "    results[act] = {'model': model, 'accuracy': accuracy}\n",
    "    print(f\"{act}å‡†ç¡®ç‡: {accuracy:.2%}\")\n",
    "\n",
    "# ç»˜åˆ¶æŸå¤±æ›²çº¿å¯¹æ¯”\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, act in enumerate(activations):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(results[act]['model'].losses)\n",
    "    plt.title(f'{act} (å‡†ç¡®ç‡: {results[act][\"accuracy\"]:.0%})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ æ€»ç»“\n",
    "\n",
    "é€šè¿‡æœ¬Notebookï¼Œæˆ‘ä»¬ï¼š\n",
    "\n",
    "1. âœ… **ä»é›¶å®ç°äº†æ„ŸçŸ¥æœº**ï¼Œç†è§£äº†å®ƒçš„å±€é™æ€§ï¼ˆåªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜ï¼‰\n",
    "2. âœ… **æ‰‹åŠ¨æ¨å¯¼å¹¶å®ç°äº†åå‘ä¼ æ’­ç®—æ³•**ï¼Œæ·±å…¥ç†è§£æ¢¯åº¦è®¡ç®—è¿‡ç¨‹\n",
    "3. âœ… **å®ç°äº†MLP**ï¼ŒæˆåŠŸè§£å†³äº†XORé—®é¢˜å’Œèºæ—‹æ•°æ®é›†\n",
    "4. âœ… **å¯è§†åŒ–äº†ä¸åŒæ¿€æ´»å‡½æ•°**çš„ç‰¹æ€§åŠå…¶å¯¼æ•°\n",
    "5. âœ… **å¯¹æ¯”äº†ä¸åŒæ¿€æ´»å‡½æ•°**åœ¨ç›¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "- **å•å±‚æ„ŸçŸ¥æœº**: åªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜\n",
    "- **MLP**: é€šè¿‡å¢åŠ éšè—å±‚å¼•å…¥éçº¿æ€§ï¼Œå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°\n",
    "- **åå‘ä¼ æ’­**: ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ï¼Œæ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•\n",
    "- **æ¿€æ´»å‡½æ•°**: ReLUåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯é»˜è®¤é€‰æ‹©ï¼ˆè®¡ç®—å¿«ã€ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼‰\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- ğŸ“˜ ç»§ç»­å­¦ä¹  `02-pytorch-basics.ipynb`ï¼Œä½¿ç”¨PyTorché‡æ–°å®ç°ä¸Šè¿°å†…å®¹\n",
    "- ğŸ¯ å°è¯•è°ƒæ•´è¶…å‚æ•°ï¼ˆéšè—å±‚å¤§å°ã€å­¦ä¹ ç‡ã€æ¿€æ´»å‡½æ•°ï¼‰è§‚å¯Ÿæ•ˆæœ\n",
    "- ğŸ“š æ·±å…¥é˜…è¯» Deep Learning Book ç¬¬6ç« ï¼šæ·±åº¦å‰é¦ˆç½‘ç»œ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
